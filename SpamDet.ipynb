{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a75807c",
   "metadata": {},
   "source": [
    "# Laboratorio : Detección de Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b88ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importar las librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import unidecode\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cae262",
   "metadata": {},
   "source": [
    "## Parte 1: Ingeniería de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb6a2d",
   "metadata": {},
   "source": [
    "### Exploración de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d662f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------D-A-T-A-S-E-T--1-------\n",
      "    Unnamed: 0                                               Body  Label\n",
      "0           0  \\r\\nSave up to 70% on Life Insurance.\\r\\nWhy S...      1\n",
      "1           1  1) Fight The Risk of Cancer!\\r\\nhttp://www.adc...      1\n",
      "2           2  1) Fight The Risk of Cancer!\\r\\nhttp://www.adc...      1\n",
      "3           3  ##############################################...      1\n",
      "4           4  I thought you might like these:\\r\\n1) Slim Dow...      1 \n",
      "-------------------------------------\n",
      "-------D-A-T-A-S-E-T--2-------\n",
      "    Unnamed: 0.1  Unnamed: 0  \\\n",
      "0          2469        2469   \n",
      "1          5063        5063   \n",
      "2         12564       12564   \n",
      "3          2796        2796   \n",
      "4          1468        1468   \n",
      "\n",
      "                                                Body  Label  \n",
      "0  Subject: stock promo mover : cwtd\\r\\n * * * ur...      1  \n",
      "1  Subject: are you listed in major search engine...      1  \n",
      "2  Subject: important information thu , 30 jun 20...      1  \n",
      "3  Subject: = ? utf - 8 ? q ? bask your life with...      1  \n",
      "4  Subject: \" bidstogo \" is places to go , things...      1   \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Cargar el dataset proporcionado\n",
    "ds1 = pd.read_csv('./Datasets/completeSpamAssassin.csv')\n",
    "ds2 = pd.read_csv('./Datasets/enronSpamSubset.csv')\n",
    "\n",
    "print('-------D-A-T-A-S-E-T--1-------\\n', ds1.head(), '\\n-------------------------------------')\n",
    "print('-------D-A-T-A-S-E-T--2-------\\n', ds2.head(), '\\n-------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4255adb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------D-A-T-A-S-E-T--F-I-N-A-L-------\n",
      "                                                 Body  Label\n",
      "0  \\r\\nSave up to 70% on Life Insurance.\\r\\nWhy S...      1\n",
      "1  1) Fight The Risk of Cancer!\\r\\nhttp://www.adc...      1\n",
      "2  1) Fight The Risk of Cancer!\\r\\nhttp://www.adc...      1\n",
      "3  ##############################################...      1\n",
      "4  I thought you might like these:\\r\\n1) Slim Dow...      1 \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Unificación de tablas\n",
    "\n",
    "# Limpieza dataset 1\n",
    "columns = list(ds1)\n",
    "columns.pop(0)\n",
    "ds1 = ds1[columns]\n",
    "\n",
    "# Limpieza dataset 2\n",
    "columns = list([0,1])\n",
    "ds2.drop(ds2.columns[[i for i in columns]], axis=1, inplace=True)\n",
    "\n",
    "# Concatenación de datasets\n",
    "df = pd.concat([ds1, ds2])\n",
    "df = df.reset_index(drop=True)\n",
    "print('-------D-A-T-A-S-E-T--F-I-N-A-L-------\\n', df.head(), '\\n-------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46146657",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac4eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    \n",
    "    # Convierte todo a str\n",
    "    if type(text) in [float, int]:\n",
    "        text = str(text)\n",
    "    elif type(text) != str:\n",
    "        return None\n",
    "    \n",
    "    # Todo en minúsculas\n",
    "    text = text.lower()\n",
    "    # Remueve acentos\n",
    "    text = unidecode.unidecode(text)\n",
    "    \n",
    "    # Tokeniza el texto en palabras\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Elimina stop words del texto\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Unifica el texto filtrado\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    filtered_text = \" \".join(filtered_words)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c31f432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Body  Label\n",
      "10205  subject : need lift support bra ! 1903 guarant...      1\n",
      "9206   subject : bought new cd ? see evidence e - mai...      1\n",
      "5266   instead spreading bbc fud , 's best go straigh...      0\n",
      "3344   http : //www.hughes-family.org/bugzilla/show_b...      0\n",
      "5962                                               empty      0\n"
     ]
    }
   ],
   "source": [
    "# Aplicación de las funciones construídas\n",
    "\n",
    "df['Body'] = df['Body'].apply(preprocessing)\n",
    "df.to_csv('./Datasets/text_filtered.csv', index=False)\n",
    "print(df.sample(n=5).head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2e530ee",
   "metadata": {},
   "source": [
    "### Representación de Texto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "700dd884",
   "metadata": {},
   "source": [
    "### Modelo Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92975aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        save 70 % life insurance . spend ? life quote ...\n",
       "1        1 ) fight risk cancer ! http : //www.adclick.w...\n",
       "2        1 ) fight risk cancer ! http : //www.adclick.w...\n",
       "3        # # # # # # # # # # # # # # # # # # # # # # # ...\n",
       "4        thought might like : 1 ) slim - guaranteed los...\n",
       "                               ...                        \n",
       "16041    subject : monday 22 nd oct louise , half hour ...\n",
       "16042    subject : missing bloomberg deals stephanie - ...\n",
       "16043    subject : eops salary survey questionnaire nee...\n",
       "16044    subject : q 3 comparison hi louise , compariso...\n",
       "16045    subject : confidential folder safely pass info...\n",
       "Name: Body, Length: 16046, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "dBoW = pd.read_csv('./Datasets/text_filtered.csv')\n",
    "dBoW = dBoW['Body']\n",
    "dBoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e0e1432",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 9.94 GiB for an array with shape (16046,) and data type <U166277",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\diego\\Documents\\Archivos\\Noveno Semestre\\Security DS\\Spam-Detection\\SpamDet.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dBoW \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(dBoW, dtype\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dBoW\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 9.94 GiB for an array with shape (16046,) and data type <U166277"
     ]
    }
   ],
   "source": [
    "dBoW = np.array(dBoW, dtype=str)\n",
    "dBoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3a169e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a487f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['save life insurance spend life quote savings ensuring family financial security important life quote savings makes buying life insurance simple affordable provide free access best companies lowest rateslife quote savings fast easy saves money let us help get started best values country new coverage save hundreds even thousands dollars requesting free quote lifequote savings service take less minutes complete shop compare save types life insurance click free quote protecting family best investment ever make receipt email error andor wish removed list please click type remove reside state prohibits email solicitations insurance please disregard email',\n",
       "       'fight risk cancer http wwwadclickwspcfm spk slim guaranteed lose lbs days http wwwadclickwspcfm spk get child support deserve free legal advice http wwwadclickwspcfm spk join web fastest growing singles community http wwwadclickwspcfm spk start private photo album online http wwwadclickwspcfm spkhave wonderful day offer manager prizemamaif wish leave list please use link http wwwqvescomtrim ilug linuxie c c irish linux users group ilug linuxie http wwwlinuxiemailmanlistinfoilug un subscription information list maintainer listmaster linuxie',\n",
       "       'fight risk cancer http wwwadclickwspcfm spk slim guaranteed lose lbs days http wwwadclickwspcfm spk get child support deserve free legal advice http wwwadclickwspcfm spk join web fastest growing singles community http wwwadclickwspcfm spk start private photo album online http wwwadclickwspcfm spkhave wonderful day offer manager prizemamaif wish leave list please use link http wwwqvescomtrim zzzz spamassassintaintorg c c',\n",
       "       ...,\n",
       "       'subject eops salary survey questionnaire need establish deadline friday march th work forwarded kim melodick hou ect pm kim melodick pm bob shults hou ect ect robert superty hou ect ect leslie reeves hou ect ect sheila glover hou ect ect kristin albrecht hou ect ect brenda f herod hou ect ect todd hall hou ect ect brent price hou ect ect michael e moscoso hou ect ect stephen p schwarz hou ect ect mary solmonson hou ect ect scott pleus hou ect ect cc sally beck hou ect ect norma villarreal hou ect ect yvonne laing hou ect ect subject eops salary survey questionnaire attached find salary survey questionnaire review need fill questionnaire exempt job group department please specific possible ensure capturing skills needed accurately price job filled questionnaire past may want update information happy address questions regarding form',\n",
       "       'subject q comparison hi louise comparison first two weeks q still early course hard say much going add june make little meaningful q stuff alone thanks jay',\n",
       "       'subject confidential folder safely pass information arthur andersen become increasingly concerned confidential information dpr position info curves validations stress tests etc passed arthur andersen audit purposes web arthur andersen email addresses necessary longer access enron internal email system please use folder described passing info would concerns picked third party via shared drive set specific purpose note aa also use shared drive pass info back questions data needs updating also consider sensitivity audit findings special presentations distributed electronically please pass note others groups need pass info back forth details access use method pass info secured folder set drive corporate called arthur andersen corporate arthur anderson please post confidential files folder rather emailing files company email address need access folder submit erequest central site http itcentral enron com data services securityrequests arthur andersen able retrieve files review terminal server access three allen center location please contact vanessa schulte problems questions beth apollo'],\n",
       "      dtype='<U165975')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(dBoW)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5db42aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 2, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [1, 3, 0, ..., 1, 0, 1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=0.2, max_df=0.8)\n",
    "# Min_df y Max_df nos sirven para controlar el porcentaje mínimo y máximo de apariciones de un token\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix = cv_matrix.astype(np.int32)\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64997671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>com</th>\n",
       "      <th>email</th>\n",
       "      <th>get</th>\n",
       "      <th>http</th>\n",
       "      <th>list</th>\n",
       "      <th>new</th>\n",
       "      <th>one</th>\n",
       "      <th>please</th>\n",
       "      <th>subject</th>\n",
       "      <th>time</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16041</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16042</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16043</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16044</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16045</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16046 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       com  email  get  http  list  new  one  please  subject  time  would\n",
       "0        0      3    1     0     1    1    0       2        0     0      0\n",
       "1        0      0    1     7     2    0    0       1        0     0      0\n",
       "2        0      0    1     6     1    0    0       1        0     0      0\n",
       "3        0      1    2     6     0    1    3       1        0     0      0\n",
       "4        0      0    1     5     2    0    0       1        0     0      0\n",
       "...    ...    ...  ...   ...   ...  ...  ...     ...      ...   ...    ...\n",
       "16041    0      0    0     0     0    0    0       0        1     0      0\n",
       "16042    0      0    0     0     0    0    1       1        7     5      0\n",
       "16043    0      0    0     0     0    0    0       1        2     0      0\n",
       "16044    0      0    0     0     0    0    0       0        1     0      0\n",
       "16045    1      3    0     1     0    0    0       4        1     0      1\n",
       "\n",
       "[16046 rows x 11 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras únicas del corpus\n",
    "vocabulario = cv.get_feature_names_out()\n",
    "# Mostrar el vector\n",
    "pd.DataFrame(cv_matrix, columns=vocabulario)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bd7084c",
   "metadata": {},
   "source": [
    "## Modelo Bag of N - grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "806dec2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 130. GiB for an array with shape (16046, 1088458) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\diego\\Documents\\Archivos\\Noveno Semestre\\Security DS\\Spam-Detection\\SpamDet.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bv \u001b[39m=\u001b[39m CountVectorizer(ngram_range\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bv_matrix \u001b[39m=\u001b[39m bv\u001b[39m.\u001b[39mfit_transform(norm_corpus)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m bv_matrix \u001b[39m=\u001b[39m bv_matrix\u001b[39m.\u001b[39;49mtoarray()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m vocabulario \u001b[39m=\u001b[39m bv\u001b[39m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pd\u001b[39m.\u001b[39mDataFrame(bv_matrix, columns\u001b[39m=\u001b[39mvocabulario)\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\sparse\\compressed.py:1039\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 1039\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[0;32m   1040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[0;32m   1041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\sparse\\base.py:1202\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m   1201\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1202\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 130. GiB for an array with shape (16046, 1088458) and data type int64"
     ]
    }
   ],
   "source": [
    "bv = CountVectorizer(ngram_range=(2,2))\n",
    "bv_matrix = bv.fit_transform(norm_corpus)\n",
    "\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "vocabulario = bv.get_feature_names_out()\n",
    "pd.DataFrame(bv_matrix, columns=vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24d41ec3",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 15.9 GiB for an array with shape (16046, 133269) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\diego\\Documents\\Archivos\\Noveno Semestre\\Security DS\\Spam-Detection\\SpamDet.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tv \u001b[39m=\u001b[39m TfidfVectorizer(min_df\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m, max_df\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m, use_idf\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tv_matrix \u001b[39m=\u001b[39m tv\u001b[39m.\u001b[39mfit_transform(norm_corpus)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tv_matrix \u001b[39m=\u001b[39m tv_matrix\u001b[39m.\u001b[39;49mtoarray()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m vocabulario \u001b[39m=\u001b[39m tv\u001b[39m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pd\u001b[39m.\u001b[39mDataFrame(np\u001b[39m.\u001b[39mround(tv_matrix, \u001b[39m2\u001b[39m), columns\u001b[39m=\u001b[39mvocabulario)\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\sparse\\compressed.py:1039\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 1039\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[0;32m   1040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[0;32m   1041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\sparse\\base.py:1202\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m   1201\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1202\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 15.9 GiB for an array with shape (16046, 133269) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocabulario = tv.get_feature_names_out()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30645d8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 15.9 GiB for an array with shape (16046, 133269) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\diego\\Documents\\Archivos\\Noveno Semestre\\Security DS\\Spam-Detection\\SpamDet.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m cosine_similarity\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m similarity_matrix \u001b[39m=\u001b[39m cosine_similarity(tv_matrix)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m similarity_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(similarity_matrix)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego/Documents/Archivos/Noveno%20Semestre/Security%20DS/Spam-Detection/SpamDet.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m similarity_df\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1253\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[1;32m-> 1253\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1254\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n\u001b[0;32m   1255\u001b[0m     Y_normalized \u001b[39m=\u001b[39m X_normalized\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1792\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1790\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not a supported axis\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m axis)\n\u001b[1;32m-> 1792\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1793\u001b[0m     X,\n\u001b[0;32m   1794\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49msparse_format,\n\u001b[0;32m   1795\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1796\u001b[0m     estimator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthe normalize function\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1797\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m   1798\u001b[0m )\n\u001b[0;32m   1799\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1800\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:821\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    815\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m feature(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m a minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m             \u001b[39m%\u001b[39m (n_features, array\u001b[39m.\u001b[39mshape, ensure_min_features, context)\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    820\u001b[0m \u001b[39mif\u001b[39;00m copy \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[1;32m--> 821\u001b[0m     array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(array, dtype\u001b[39m=\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n\u001b[0;32m    823\u001b[0m \u001b[39mreturn\u001b[39;00m array\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 15.9 GiB for an array with shape (16046, 133269) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(tv_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "74fbae03f09acb54b4b85bc5f11519af67119ba3a464556648626baa681cdd43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
